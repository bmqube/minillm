# MiniLLM ðŸ¤–

A lightweight, efficient transformer inference engine written in Rust. MiniLLM provides a clean, well-documented implementation of GPT-2 style transformer models with support for text generation.

## âœ¨ Features

- **ï¿½ Fast Inference**: Efficient tensor operations using ndarray
- **ðŸ”’ Memory Safe**: Written in Rust with zero-copy operations where possible  
- **ðŸ“¦ Easy to Use**: High-level API for quick integration
- **ðŸŽ¯ Well Tested**: Comprehensive examples and documentation
- **ðŸ”§ Extensible**: Modular architecture for easy customization

## ðŸ—ï¸ Architecture

```
src/
â”œâ”€â”€ lib.rs          # Library entry point and public API
â”œâ”€â”€ main.rs         # Simple CLI example
â”œâ”€â”€ inference.rs    # High-level inference engine
â”œâ”€â”€ gpt.rs          # GPT model implementation
â”œâ”€â”€ transformer.rs  # Transformer block components
â”œâ”€â”€ attention.rs    # Multi-head attention mechanism
â”œâ”€â”€ mlp.rs          # Feed-forward network layers
â”œâ”€â”€ tensor.rs       # Tensor operations and math
â”œâ”€â”€ weights.rs      # Model weight loading (SafeTensors)
â””â”€â”€ config.rs       # Model configuration handling

examples/
â”œâ”€â”€ basic_generation.rs  # Simple text generation
â”œâ”€â”€ interactive_chat.rs  # Interactive chat interface
â””â”€â”€ tokenization.rs      # Tokenization examples
```

## ðŸš€ Quick Start

### Library Usage

```rust
use minillm::inference::InferenceEngine;

fn main() -> minillm::Result<()> {
    // Load a GPT-2 model
    let engine = InferenceEngine::new("openai-community/gpt2")?;
    
    // Generate text
    let prompt = "The future of AI is";
    let generated = engine.generate(prompt, 20)?;
    
    println!("Generated: {}", generated);
    Ok(())
}
```

MiniLLM is a lightweight inference engine designed for running language models efficiently. The goal is to provide a simple, fast, and memory-efficient solution for LLM inference.

## Features (Planned)

- [ ] Model loading and inference
- [ ] Support for multiple model formats
- [ ] Memory-efficient execution
- [ ] CPU and GPU acceleration
- [ ] Simple API for integration

## Installation

This crate is not yet published to crates.io. Once available, you can install it with:

```bash
cargo add minillm
```

## Usage

Documentation and usage examples will be provided as the project develops.

## Contributing

Contributions are welcome! Please feel free to submit issues and pull requests.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Author

BM Monjur Morshed
